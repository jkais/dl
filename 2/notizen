SETTING UP YOUR APP

Train / Dev / Test Sets:
- train set
- hold-out cross validation / development set
- test set

- Früher 60/20/20 Ratio
- Big Data Trend: Bei Millionen Daten wäre z.B. 98/1/1 okay oder gar noch mehr

- Dev, Train und Test Sets aus der selben Datenquelle, möglichst nah am realen Einsatzzweck (z.B. keine Agentur-Bilder, wenn User Content später verwendet wird)

Bias and Variance
- Overfitting vs Underfitting
- Train Set error and Dev Set error
- niedriger Training Set Error und hoher Dev Set Error: Overfitting, High Variance
- hoher Training Set Error und ähnlich hoher Dev Set Error: Underfitting, High Bias
- hoher Training Set Error und viel höherer Dev Set Error: High Bias, High Variance
- niedriger TSE, niedriger DSE: Low Bias, Low Variance

Basic Recipe for ML
1. High Bias? (= training data performance) -> Bigger Network, Train longer, maybe change architecture
2. High Variance? (= dev set performance) -> More data, Regularization, maybe change architecture

- Früher hat Reduzierung des Bias die Variance erhöht und umgekehrt, heute eigentlich nicht mehr


REGULARIZATION
Bei Linear Regression:
- L2 regularization, lambda/2m * euclidian norm of w, summe von w²
- euclidian norm: np.dot(w.T, w)
- L1 regularization: lambda/2m  * summe von |w|
- Lambda - regularization parameter, ist ein Hyperparameter

Im Neural Network:
- Wie L2 bei LR, nur halt als Summe über alle W
- Frobenius Norm
- In der Backpropagation landet ein neuer Therm: lambda/m * W
- Beim Updaten der Weights wird also alpha*lambda/m * W abgezogen. Deswegen: Weight Deca

- Warum löst L2 Regularization Overfitting? Es erzwingt kleinere Ws. Dadurch machen die einzelnen Layer des Netzes nicht mehr so komplizierte Dinge.
- Bei z.B. tanh als Aktivierungsfunktion sind die Werte für kleine Ws nahe 0 und somit quasi linear. Je mehr Layer quasi linear arbeiten, umso mehr ist das gesamte Netz quasi linear

- Achtung: Durch Hinzufügen der Frobenius-Norm in der Kostenfunktion lässt sich nicht mehr zwingend eine von Iteration zu Iteration monoton fallende Kostenfunktion beobachten!

Dropouts:
- Mit einer gewissen Wahrscheinlichkeit (z.B. 50%) wird eine Node aus dem Netz geworfen
Inverted Dropout:
- Erstelle für jeden Trainingslauf eine Dropout-Matrix d pro Layer
- np.random.rand(a.shape[0], a.shape[1] < keep_probability # keep_probability z.B. 0.8
- a = np.multiply(a, d) # Setze alle Werte von a auf 0, wo d false ist
- a /= keep_probability # Erhöhe noch vorhandene Werte von a, damit der Gesamt-Output des Layers prozentual gleich bleibt
- Dropout nur in Training, nicht in Test -> keep_probability = 1.0

- Eine Node kann sich nicht mehr nur auf einen starken Vorgänger verlassen, muss die Weights verteilen

- keep_prob pro Layer durchaus sinnvoll
- Nur wenn man Overfitting-Probleme hat, machen Dropouts Sinn (z.B. Computer Vision)

andere Techniken:
- Data Augmentation: z.B. Bilder horizontal spiegeln, zoomen, rotieren...
- Early Stopping: Dev Set Error betrachten, sobald der sich erhöht, anhalten; Als zusätzlicher Hyperparameter macht es die Sache aber nicht leichter

SETUP
Normalization:
- Subtract the mean: Durchschnitt ausrechnen, von allen Werten abziehen => Normiert die Werte rund um 0
- Normalize variances: variance = 1/m sum xi²; x /= variance
- Normalisieren von Training und Dev/Test mit den gleichen Werten
- Linear Gradient auf der Kostenfunktion wird viel einfacher

Vanishing / Exploding Gradients: Tiefe Netze können Ergebnisse exponentiell groß oder klein werden lassen
- Lösung Weight Initialization
- Var(w) = 1/n (n = Anzahl der Eingänge), W = np.random.randn(shape) * np.sqrt(1/n)
- Bei RELU var => 2/n besser
- tanh var => 1/n (Xavier Initialization)

Gradient Checking:
- W1, b1, W2, b2, ..., WL, bL in einen großen Vektor Theta konkatenieren
- Ebenso dW1, db1, ..., dWL, dbL
- Für jedes i: dApproxt i = J(t1, t2, ..., ti + e, ti+1,...) - J(t1, ..., ti - e, ...) / 2e
- Ist dApproxt ungefähr gleich dt?
- Euklidischer Abstand: ||dApproxt - dt||2  # ||x - y||2 = sqrt sum (xi - yi)²
- Den teilen durch die Summe der Länge beider Matritzen
- e = 10^-7 ist ganz gut
- Dann sollten die Werte im Bereich 10^-7 liegen; spätestens bei 10^-3 gibt es wohl einen Bug


INITIALIZATION:

He Initialization:

    for l in range(1, L + 1):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
        ### END CODE HERE ###


Tipps zu Grad Checking:
- Nicht im regulären Training, nur zum Debuggen
- Wenn auffällig, Theta angucken, welche Werte sind betroffen?
- Regularization nicht vergessen!
- Grad Checking geht nicht mit Dropouts!


