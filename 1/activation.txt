sigmoid(z) = 1/(1 + e^-z)
tanh(z) = (e^z - e^-z)/(e^z + e^-z)
relu(z) = max(0,z)
leaky_relu(z) = max(z*0.01,z)

Sigmoid nur im letzten Layer, wenn man binäre Ergebnisse hat

Lineare Aktivierungsfunktionen sorgen dafür, dass die hidden layer voneinander abhängig
sind und man so eigentlich nur eine einfache lineare Regression berechnet.

Ableitungen
sigmoid'(z) = 1/(1+e^-z)*(1 - 1/(1+e^-z)) = sigmoid(z)*(1 - sigmoid(z))
tanh'(z) = 1 - tanh^2(z)
relu'(z) = 0 if z < 0; 1 if z > 0; undefined if z == 0 (aber es ist voll okay, z==0 auf 0 oder 1 zu setzen, weil egal)
leaky_relu'(z) = 0.01 if z < 0; 1 if z > 0; undefined if z == 0 (aber es ist voll okay, z==0 auf 0.01 oder 1 zu setzen, weil egal)
